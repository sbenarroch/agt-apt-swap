{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraper for `tauschwohnung.de`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import yaml\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_URL = 'https://www.tauschwohnung.com'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGIN_URL = 'https://www.tauschwohnung.com/login'\n",
    "\n",
    "cookies = {\n",
    "    '_pk_id.1.3aa3': 'efce13a6140ddc5c.1683740292',\n",
    "    'tauschwohnung': 'f5857706a5a85b28ec1a97a517ce8e478158582b',\n",
    "    '_pk_ses.1.3aa3': '%2A',\n",
    "    '_pk_cvar.1.3aa3': '%5B%5D',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/113.0',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Content-Type': 'application/x-www-form-urlencoded',\n",
    "    'Origin': 'https://www.tauschwohnung.com',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Referer': 'https://www.tauschwohnung.com/login',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'same-origin',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'TE': 'trailers',\n",
    "}\n",
    "\n",
    "payload = yaml.safe_load('./credentials/tauschwohnung.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = requests.session()\n",
    "login_req = s.post(LOGIN_URL, headers=headers, cookies=cookies, data=payload)\n",
    "\n",
    "if login_req.status_code != 200:\n",
    "    raise ValueError('Login failed')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_URL = 'https://www.tauschwohnung.com/search/result?city={city}&housing_type=1&rooms_min=1&storey_min=-1&storey_max=10&sort=standard&page={page}'\n",
    "\n",
    "cities = [\n",
    "    'Berlin',\n",
    "    'Köln',\n",
    "    'München',\n",
    "    'Zürich',\n",
    "    'Hamburg',\n",
    "    'Wien',\n",
    "    'Düsseldorf',\n",
    "    'Frankfurt+am+Main',\n",
    "    'Stuttgart'\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stuttgart test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Stuttgart'\n",
    "\n",
    "soup = BeautifulSoup(s.get(SEARCH_URL.format(city=city, page=1)).text, 'html.parser')\n",
    "\n",
    "page_links = soup.find_all('a', class_ = 'page-link')\n",
    "max_page = int(page_links[-2].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for page in range(1, 5):\n",
    "    soup = BeautifulSoup(s.get(SEARCH_URL.format(city=city, page=page)).text, 'html.parser')\n",
    "    for link in soup.find_all('a', class_ = 'stretched-link embed-responsive embed-responsive-4by3'):\n",
    "        links.append(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'g-city': [],\n",
    "    'g-rent': [],\n",
    "    'g-surf': [],\n",
    "    'g-rooms': [],\n",
    "    't-city': [],\n",
    "    't-max-rent': [],\n",
    "    't-surf-min': [],\n",
    "    't-rooms': []\n",
    "}\n",
    "\n",
    "for link in links:\n",
    "    soup = BeautifulSoup(s.get(ROOT_URL + link).text, 'html.parser')\n",
    "    strong = soup.find_all('strong')\n",
    "    data['g-city'].append(city)\n",
    "    data['g-rent'].append(strong[0].text)\n",
    "    data['g-surf'].append(strong[1].text)\n",
    "    data['g-rooms'].append(strong[2].text)\n",
    "    data['t-city'].append(strong[5].text.strip())\n",
    "    data['t-max-rent'].append(strong[7].text.strip())\n",
    "    data['t-surf-min'].append(strong[8].text.strip())\n",
    "    data['t-rooms'].append(strong[9].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'g-city': [],\n",
    "    'g-rent': [],\n",
    "    'g-surf': [],\n",
    "    'g-rooms': [],\n",
    "    't-city': [],\n",
    "    't-max-rent': [],\n",
    "    't-surf-min': [],\n",
    "    't-rooms': []\n",
    "}\n",
    "\n",
    "for city in cities:\n",
    "    # Find how many pages we have to browse\n",
    "    soup = BeautifulSoup(s.get(SEARCH_URL.format(city=city, page=1)).text, 'html.parser')\n",
    "    page_links = soup.find_all('a', class_ = 'page-link')\n",
    "    max_page = int(page_links[-2].text)\n",
    "\n",
    "    # Store the links to browse\n",
    "    links = []\n",
    "    for page in range(1, max_page + 1):\n",
    "        soup = BeautifulSoup(s.get(SEARCH_URL.format(city=city, page=page)).text, 'html.parser')\n",
    "        for link in soup.find_all('a', class_ = 'stretched-link embed-responsive embed-responsive-4by3'):\n",
    "            links.append(link['href'])\n",
    "\n",
    "    # Browse the links and store data\n",
    "    for link in links:\n",
    "        soup = BeautifulSoup(s.get(ROOT_URL + link).text, 'html.parser')\n",
    "        strong = soup.find_all('strong')\n",
    "        data['g-city'].append(city)\n",
    "        data['g-rent'].append(strong[0].text)\n",
    "        data['g-surf'].append(strong[1].text)\n",
    "        data['g-rooms'].append(strong[2].text)\n",
    "        data['t-city'].append(strong[5].text.strip())\n",
    "        data['t-max-rent'].append(strong[7].text.strip())\n",
    "        data['t-surf-min'].append(strong[8].text.strip())\n",
    "        data['t-rooms'].append(strong[9].text.strip())\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
